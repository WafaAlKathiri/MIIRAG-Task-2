{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b578f43",
   "metadata": {
    "papermill": {
     "duration": 0.003978,
     "end_time": "2025-02-17T06:58:58.129377",
     "exception": false,
     "start_time": "2025-02-17T06:58:58.125399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# SETUP "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f9e2b3",
   "metadata": {
    "id": "JIHuj2eEy13f",
    "papermill": {
     "duration": 0.003162,
     "end_time": "2025-02-17T06:58:58.136131",
     "exception": false,
     "start_time": "2025-02-17T06:58:58.132969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Install Stylegan2-ada-pytorch Prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc8819fd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T06:58:58.143736Z",
     "iopub.status.busy": "2025-02-17T06:58:58.143440Z",
     "iopub.status.idle": "2025-02-17T07:00:59.787892Z",
     "shell.execute_reply": "2025-02-17T07:00:59.786889Z"
    },
    "id": "zgRYoB3SatQ7",
    "outputId": "699cc3ac-81a0-479b-f606-f03ac7b8b17e",
    "papermill": {
     "duration": 121.650029,
     "end_time": "2025-02-17T07:00:59.789585",
     "exception": false,
     "start_time": "2025-02-17T06:58:58.139556",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\r\n",
      "Collecting torch===1.13.0\r\n",
      "  Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl.metadata (23 kB)\r\n",
      "Collecting torchvision===0.1.6\r\n",
      "  Downloading https://download.pytorch.org/whl/torchvision-0.1.6-py3-none-any.whl (16 kB)\r\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch===1.13.0) (4.12.2)\r\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99 (from torch===1.13.0)\r\n",
      "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96 (from torch===1.13.0)\r\n",
      "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66 (from torch===1.13.0)\r\n",
      "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99 (from torch===1.13.0)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch===1.13.0) (75.1.0)\r\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch===1.13.0) (0.45.1)\r\n",
      "Downloading torch-1.13.0-cp310-cp310-manylinux1_x86_64.whl (890.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m890.1/890.1 MB\u001b[0m \u001b[31m263.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m275.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m267.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m363.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m310.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: torchvision, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, nvidia-cudnn-cu11, torch\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.20.1+cu121\r\n",
      "    Uninstalling torchvision-0.20.1+cu121:\r\n",
      "      Successfully uninstalled torchvision-0.20.1+cu121\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.5.1+cu121\r\n",
      "    Uninstalling torch-2.5.1+cu121:\r\n",
      "      Successfully uninstalled torch-2.5.1+cu121\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "easyocr 1.7.2 requires torchvision>=0.5, but you have torchvision 0.1.6 which is incompatible.\r\n",
      "fastai 2.7.18 requires torchvision>=0.11, but you have torchvision 0.1.6 which is incompatible.\r\n",
      "pytorch-lightning 2.5.0.post0 requires torch>=2.1.0, but you have torch 1.13.0 which is incompatible.\r\n",
      "torchaudio 2.5.1+cu121 requires torch==2.5.1, but you have torch 1.13.0 which is incompatible.\r\n",
      "torchmetrics 1.6.1 requires torch>=2.0.0, but you have torch 1.13.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 torch-1.13.0 torchvision-0.1.6\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir torch===1.13.0 torchvision===0.1.6 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d4e7c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:00:59.811547Z",
     "iopub.status.busy": "2025-02-17T07:00:59.811287Z",
     "iopub.status.idle": "2025-02-17T07:01:01.133245Z",
     "shell.execute_reply": "2025-02-17T07:01:01.132598Z"
    },
    "id": "BhPcRJ7Jc6hf",
    "papermill": {
     "duration": 1.334568,
     "end_time": "2025-02-17T07:01:01.134782",
     "exception": false,
     "start_time": "2025-02-17T07:00:59.800214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11a7206",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:01.156825Z",
     "iopub.status.busy": "2025-02-17T07:01:01.156514Z",
     "iopub.status.idle": "2025-02-17T07:01:01.170133Z",
     "shell.execute_reply": "2025-02-17T07:01:01.169586Z"
    },
    "id": "s123TuFmc_hx",
    "papermill": {
     "duration": 0.025694,
     "end_time": "2025-02-17T07:01:01.171349",
     "exception": false,
     "start_time": "2025-02-17T07:01:01.145655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07c51d4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:01.192766Z",
     "iopub.status.busy": "2025-02-17T07:01:01.192554Z",
     "iopub.status.idle": "2025-02-17T07:01:01.196201Z",
     "shell.execute_reply": "2025-02-17T07:01:01.195504Z"
    },
    "papermill": {
     "duration": 0.01571,
     "end_time": "2025-02-17T07:01:01.197398",
     "exception": false,
     "start_time": "2025-02-17T07:01:01.181688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3acf9f",
   "metadata": {
    "id": "2y195-nOz19D",
    "papermill": {
     "duration": 0.009844,
     "end_time": "2025-02-17T07:01:01.217535",
     "exception": false,
     "start_time": "2025-02-17T07:01:01.207691",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Verify the next command results in \"1\".  If not, go back to the beginning and verify you have a GPU runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0ffae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:01.238663Z",
     "iopub.status.busy": "2025-02-17T07:01:01.238435Z",
     "iopub.status.idle": "2025-02-17T07:01:01.266792Z",
     "shell.execute_reply": "2025-02-17T07:01:01.266001Z"
    },
    "id": "KV07xnKTprut",
    "outputId": "c65e9351-9622-4873-9f03-3c76455e72b1",
    "papermill": {
     "duration": 0.040531,
     "end_time": "2025-02-17T07:01:01.268098",
     "exception": false,
     "start_time": "2025-02-17T07:01:01.227567",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a3a5591",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:01.289309Z",
     "iopub.status.busy": "2025-02-17T07:01:01.289052Z",
     "iopub.status.idle": "2025-02-17T07:01:06.680105Z",
     "shell.execute_reply": "2025-02-17T07:01:06.679030Z"
    },
    "id": "sZhoCdj4pREI",
    "outputId": "ff90f40d-ba81-4788-af36-9db6401449bc",
    "papermill": {
     "duration": 5.403421,
     "end_time": "2025-02-17T07:01:06.681814",
     "exception": false,
     "start_time": "2025-02-17T07:01:01.278393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (8.1.7)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.67.1)\r\n",
      "Collecting pyspng\r\n",
      "  Downloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.8 kB)\r\n",
      "Requirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (1.11.1.3)\r\n",
      "Collecting imageio-ffmpeg==0.4.3\r\n",
      "  Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.3.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2025.1.31)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pyspng) (1.26.4)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (2025.0.1)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (2022.0.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->pyspng) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pyspng) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->pyspng) (2022.0.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->pyspng) (1.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->pyspng) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->pyspng) (2024.2.0)\r\n",
      "Downloading imageio_ffmpeg-0.4.3-py3-none-manylinux2010_x86_64.whl (26.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m158.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading pyspng-0.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (192 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m192.5/192.5 kB\u001b[0m \u001b[31m338.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: imageio-ffmpeg, pyspng\r\n",
      "  Attempting uninstall: imageio-ffmpeg\r\n",
      "    Found existing installation: imageio-ffmpeg 0.5.1\r\n",
      "    Uninstalling imageio-ffmpeg-0.5.1:\r\n",
      "      Successfully uninstalled imageio-ffmpeg-0.5.1\r\n",
      "Successfully installed imageio-ffmpeg-0.4.3 pyspng-0.1.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install --no-cache-dir click requests tqdm pyspng ninja imageio-ffmpeg==0.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285bdd3c",
   "metadata": {
    "id": "b94le1Kh_v1k",
    "papermill": {
     "duration": 0.010792,
     "end_time": "2025-02-17T07:01:06.704047",
     "exception": false,
     "start_time": "2025-02-17T07:01:06.693255",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Get the StyleGAN code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31221242",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:06.727136Z",
     "iopub.status.busy": "2025-02-17T07:01:06.726860Z",
     "iopub.status.idle": "2025-02-17T07:01:07.962138Z",
     "shell.execute_reply": "2025-02-17T07:01:07.961297Z"
    },
    "id": "najHDP7gMeFy",
    "outputId": "2ffa3998-0bd9-48bb-a388-825c753c5385",
    "papermill": {
     "duration": 1.248314,
     "end_time": "2025-02-17T07:01:07.963495",
     "exception": false,
     "start_time": "2025-02-17T07:01:06.715181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'stylegan2-ada-pytorch'...\r\n",
      "remote: Enumerating objects: 152, done.\u001b[K\r\n",
      "remote: Counting objects: 100% (22/22), done.\u001b[K\r\n",
      "remote: Compressing objects: 100% (22/22), done.\u001b[K\r\n",
      "remote: Total 152 (delta 10), reused 0 (delta 0), pack-reused 130 (from 2)\u001b[K\r\n",
      "Receiving objects: 100% (152/152), 1.15 MiB | 9.78 MiB/s, done.\r\n",
      "Resolving deltas: 100% (68/68), done.\r\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mariamhash/stylegan2-ada-pytorch.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b48c06f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:07.987558Z",
     "iopub.status.busy": "2025-02-17T07:01:07.987310Z",
     "iopub.status.idle": "2025-02-17T07:01:08.111549Z",
     "shell.execute_reply": "2025-02-17T07:01:08.110496Z"
    },
    "id": "GCYmJ2Z60WSe",
    "papermill": {
     "duration": 0.137645,
     "end_time": "2025-02-17T07:01:08.112882",
     "exception": false,
     "start_time": "2025-02-17T07:01:07.975237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mkdir /kaggle/working/stylegan2-ada-pytorch/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4dd5b289",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.136461Z",
     "iopub.status.busy": "2025-02-17T07:01:08.136216Z",
     "iopub.status.idle": "2025-02-17T07:01:08.143178Z",
     "shell.execute_reply": "2025-02-17T07:01:08.142344Z"
    },
    "id": "zRwQwWc9RNAD",
    "outputId": "967b50ef-98ba-4301-ccca-e08d19240a9c",
    "papermill": {
     "duration": 0.020116,
     "end_time": "2025-02-17T07:01:08.144550",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.124434",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/stylegan2-ada-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6662c8",
   "metadata": {
    "id": "QTo4K3m8AEzh",
    "papermill": {
     "duration": 0.011076,
     "end_time": "2025-02-17T07:01:08.166807",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.155731",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Create Folders on Kaggle to store the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be7bf4af",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.189784Z",
     "iopub.status.busy": "2025-02-17T07:01:08.189558Z",
     "iopub.status.idle": "2025-02-17T07:01:08.309326Z",
     "shell.execute_reply": "2025-02-17T07:01:08.308420Z"
    },
    "id": "MexWxMvLHP5O",
    "papermill": {
     "duration": 0.132867,
     "end_time": "2025-02-17T07:01:08.310769",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.177902",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/GANProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4489ad26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.334427Z",
     "iopub.status.busy": "2025-02-17T07:01:08.334183Z",
     "iopub.status.idle": "2025-02-17T07:01:08.453013Z",
     "shell.execute_reply": "2025-02-17T07:01:08.452087Z"
    },
    "id": "Tja3AaYIHU9v",
    "papermill": {
     "duration": 0.132125,
     "end_time": "2025-02-17T07:01:08.454309",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.322184",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir /kaggle/working/GANProject/Cracks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66650e6",
   "metadata": {
    "id": "ZpOwAUvJwoiY",
    "papermill": {
     "duration": 0.01162,
     "end_time": "2025-02-17T07:01:08.477443",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.465823",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dce71157",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.501059Z",
     "iopub.status.busy": "2025-02-17T07:01:08.500816Z",
     "iopub.status.idle": "2025-02-17T07:01:08.505156Z",
     "shell.execute_reply": "2025-02-17T07:01:08.504255Z"
    },
    "id": "V9pbaodn8LKY",
    "outputId": "7ebe8ccc-21f0-4817-fe4b-b18f0216107e",
    "papermill": {
     "duration": 0.0177,
     "end_time": "2025-02-17T07:01:08.506448",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.488748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/stylegan2-ada-pytorch\n"
     ]
    }
   ],
   "source": [
    "%cd /kaggle/working/stylegan2-ada-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40e81e3d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.529912Z",
     "iopub.status.busy": "2025-02-17T07:01:08.529711Z",
     "iopub.status.idle": "2025-02-17T07:01:08.532500Z",
     "shell.execute_reply": "2025-02-17T07:01:08.531888Z"
    },
    "id": "nIqAPSgaHKEZ",
    "outputId": "1793c2f1-b4ac-46af-c415-f01e86edbb22",
    "papermill": {
     "duration": 0.01593,
     "end_time": "2025-02-17T07:01:08.533616",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.517686",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pretrained_model = \"/kaggle/input/textures.pkl/other/default/3/network-snapshot-000220.pkl\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f8a596c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-17T07:01:08.557721Z",
     "iopub.status.busy": "2025-02-17T07:01:08.557467Z",
     "iopub.status.idle": "2025-02-17T18:03:40.680771Z",
     "shell.execute_reply": "2025-02-17T18:03:40.679540Z"
    },
    "papermill": {
     "duration": 39752.13764,
     "end_time": "2025-02-17T18:03:40.682889",
     "exception": false,
     "start_time": "2025-02-17T07:01:08.545249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Training options:\r\n",
      "{\r\n",
      "  \"num_gpus\": 2,\r\n",
      "  \"image_snapshot_ticks\": 5,\r\n",
      "  \"network_snapshot_ticks\": 5,\r\n",
      "  \"metrics\": [],\r\n",
      "  \"random_seed\": 0,\r\n",
      "  \"training_set_kwargs\": {\r\n",
      "    \"class_name\": \"training.dataset.ImageFolderDataset\",\r\n",
      "    \"path\": \"/kaggle/input/class6-dataset-a\",\r\n",
      "    \"use_labels\": false,\r\n",
      "    \"max_size\": 290,\r\n",
      "    \"xflip\": true,\r\n",
      "    \"resolution\": 512\r\n",
      "  },\r\n",
      "  \"data_loader_kwargs\": {\r\n",
      "    \"pin_memory\": true,\r\n",
      "    \"num_workers\": 3,\r\n",
      "    \"prefetch_factor\": 2\r\n",
      "  },\r\n",
      "  \"G_kwargs\": {\r\n",
      "    \"class_name\": \"training.networks.Generator\",\r\n",
      "    \"z_dim\": 512,\r\n",
      "    \"w_dim\": 512,\r\n",
      "    \"mapping_kwargs\": {\r\n",
      "      \"num_layers\": 8\r\n",
      "    },\r\n",
      "    \"synthesis_kwargs\": {\r\n",
      "      \"channel_base\": 32768,\r\n",
      "      \"channel_max\": 512,\r\n",
      "      \"num_fp16_res\": 4,\r\n",
      "      \"conv_clamp\": 256\r\n",
      "    }\r\n",
      "  },\r\n",
      "  \"D_kwargs\": {\r\n",
      "    \"class_name\": \"training.networks.Discriminator\",\r\n",
      "    \"block_kwargs\": {},\r\n",
      "    \"mapping_kwargs\": {},\r\n",
      "    \"epilogue_kwargs\": {\r\n",
      "      \"mbstd_group_size\": 8\r\n",
      "    },\r\n",
      "    \"channel_base\": 32768,\r\n",
      "    \"channel_max\": 512,\r\n",
      "    \"num_fp16_res\": 4,\r\n",
      "    \"conv_clamp\": 256\r\n",
      "  },\r\n",
      "  \"G_opt_kwargs\": {\r\n",
      "    \"class_name\": \"torch.optim.Adam\",\r\n",
      "    \"lr\": 0.0025,\r\n",
      "    \"betas\": [\r\n",
      "      0,\r\n",
      "      0.99\r\n",
      "    ],\r\n",
      "    \"eps\": 1e-08\r\n",
      "  },\r\n",
      "  \"D_opt_kwargs\": {\r\n",
      "    \"class_name\": \"torch.optim.Adam\",\r\n",
      "    \"lr\": 0.0025,\r\n",
      "    \"betas\": [\r\n",
      "      0,\r\n",
      "      0.99\r\n",
      "    ],\r\n",
      "    \"eps\": 1e-08\r\n",
      "  },\r\n",
      "  \"loss_kwargs\": {\r\n",
      "    \"class_name\": \"training.loss.StyleGAN2Loss\",\r\n",
      "    \"r1_gamma\": 0.5\r\n",
      "  },\r\n",
      "  \"total_kimg\": 15000,\r\n",
      "  \"batch_size\": 32,\r\n",
      "  \"batch_gpu\": 16,\r\n",
      "  \"ema_kimg\": 20,\r\n",
      "  \"ema_rampup\": null,\r\n",
      "  \"ada_target\": 0.6,\r\n",
      "  \"augment_kwargs\": {\r\n",
      "    \"class_name\": \"training.augment.AugmentPipe\",\r\n",
      "    \"xflip\": 1,\r\n",
      "    \"rotate90\": 0,\r\n",
      "    \"xint\": 0,\r\n",
      "    \"scale\": 1,\r\n",
      "    \"rotate\": 0,\r\n",
      "    \"aniso\": 0,\r\n",
      "    \"xfrac\": 1,\r\n",
      "    \"brightness\": 1,\r\n",
      "    \"contrast\": 1,\r\n",
      "    \"lumaflip\": 0,\r\n",
      "    \"hue\": 0,\r\n",
      "    \"saturation\": 0,\r\n",
      "    \"imgfilter\": 1\r\n",
      "  },\r\n",
      "  \"resume_pkl\": \"/kaggle/input/textures.pkl/other/default/3/network-snapshot-000220.pkl\",\r\n",
      "  \"ada_kimg\": 100,\r\n",
      "  \"run_dir\": \"/kaggle/working/GANProject/Cracks/00000-class6-dataset-a-mirror-paper512-kimg15000-batch32-bgcf-resumecustom\"\r\n",
      "}\r\n",
      "\r\n",
      "Output directory:   /kaggle/working/GANProject/Cracks/00000-class6-dataset-a-mirror-paper512-kimg15000-batch32-bgcf-resumecustom\r\n",
      "Training data:      /kaggle/input/class6-dataset-a\r\n",
      "Training duration:  15000 kimg\r\n",
      "Number of GPUs:     2\r\n",
      "Number of images:   290\r\n",
      "Image resolution:   512\r\n",
      "Conditional model:  False\r\n",
      "Dataset x-flips:    True\r\n",
      "\r\n",
      "Creating output directory...\r\n",
      "Launching processes...\r\n",
      "Loading training set...\r\n",
      "\r\n",
      "Num images:  580\r\n",
      "Image shape: [3, 512, 512]\r\n",
      "Label shape: [0]\r\n",
      "\r\n",
      "Constructing networks...\r\n",
      "Resuming from \"/kaggle/input/textures.pkl/other/default/3/network-snapshot-000220.pkl\"\r\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\r\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\r\n",
      "\r\n",
      "Generator             Parameters  Buffers  Output shape         Datatype\r\n",
      "---                   ---         ---      ---                  ---     \r\n",
      "mapping.fc0           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc1           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc2           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc3           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc4           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc5           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc6           262656      -        [16, 512]            float32 \r\n",
      "mapping.fc7           262656      -        [16, 512]            float32 \r\n",
      "mapping               -           512      [16, 16, 512]        float32 \r\n",
      "synthesis.b4.conv1    2622465     32       [16, 512, 4, 4]      float32 \r\n",
      "synthesis.b4.torgb    264195      -        [16, 3, 4, 4]        float32 \r\n",
      "synthesis.b4:0        8192        16       [16, 512, 4, 4]      float32 \r\n",
      "synthesis.b4:1        -           -        [16, 512, 4, 4]      float32 \r\n",
      "synthesis.b8.conv0    2622465     80       [16, 512, 8, 8]      float32 \r\n",
      "synthesis.b8.conv1    2622465     80       [16, 512, 8, 8]      float32 \r\n",
      "synthesis.b8.torgb    264195      -        [16, 3, 8, 8]        float32 \r\n",
      "synthesis.b8:0        -           16       [16, 512, 8, 8]      float32 \r\n",
      "synthesis.b8:1        -           -        [16, 512, 8, 8]      float32 \r\n",
      "synthesis.b16.conv0   2622465     272      [16, 512, 16, 16]    float32 \r\n",
      "synthesis.b16.conv1   2622465     272      [16, 512, 16, 16]    float32 \r\n",
      "synthesis.b16.torgb   264195      -        [16, 3, 16, 16]      float32 \r\n",
      "synthesis.b16:0       -           16       [16, 512, 16, 16]    float32 \r\n",
      "synthesis.b16:1       -           -        [16, 512, 16, 16]    float32 \r\n",
      "synthesis.b32.conv0   2622465     1040     [16, 512, 32, 32]    float32 \r\n",
      "synthesis.b32.conv1   2622465     1040     [16, 512, 32, 32]    float32 \r\n",
      "synthesis.b32.torgb   264195      -        [16, 3, 32, 32]      float32 \r\n",
      "synthesis.b32:0       -           16       [16, 512, 32, 32]    float32 \r\n",
      "synthesis.b32:1       -           -        [16, 512, 32, 32]    float32 \r\n",
      "synthesis.b64.conv0   2622465     4112     [16, 512, 64, 64]    float16 \r\n",
      "synthesis.b64.conv1   2622465     4112     [16, 512, 64, 64]    float16 \r\n",
      "synthesis.b64.torgb   264195      -        [16, 3, 64, 64]      float16 \r\n",
      "synthesis.b64:0       -           16       [16, 512, 64, 64]    float16 \r\n",
      "synthesis.b64:1       -           -        [16, 512, 64, 64]    float32 \r\n",
      "synthesis.b128.conv0  1442561     16400    [16, 256, 128, 128]  float16 \r\n",
      "synthesis.b128.conv1  721409      16400    [16, 256, 128, 128]  float16 \r\n",
      "synthesis.b128.torgb  132099      -        [16, 3, 128, 128]    float16 \r\n",
      "synthesis.b128:0      -           16       [16, 256, 128, 128]  float16 \r\n",
      "synthesis.b128:1      -           -        [16, 256, 128, 128]  float32 \r\n",
      "synthesis.b256.conv0  426369      65552    [16, 128, 256, 256]  float16 \r\n",
      "synthesis.b256.conv1  213249      65552    [16, 128, 256, 256]  float16 \r\n",
      "synthesis.b256.torgb  66051       -        [16, 3, 256, 256]    float16 \r\n",
      "synthesis.b256:0      -           16       [16, 128, 256, 256]  float16 \r\n",
      "synthesis.b256:1      -           -        [16, 128, 256, 256]  float32 \r\n",
      "synthesis.b512.conv0  139457      262160   [16, 64, 512, 512]   float16 \r\n",
      "synthesis.b512.conv1  69761       262160   [16, 64, 512, 512]   float16 \r\n",
      "synthesis.b512.torgb  33027       -        [16, 3, 512, 512]    float16 \r\n",
      "synthesis.b512:0      -           16       [16, 64, 512, 512]   float16 \r\n",
      "synthesis.b512:1      -           -        [16, 64, 512, 512]   float32 \r\n",
      "---                   ---         ---      ---                  ---     \r\n",
      "Total                 30276583    699904   -                    -       \r\n",
      "\r\n",
      "\r\n",
      "Discriminator  Parameters  Buffers  Output shape         Datatype\r\n",
      "---            ---         ---      ---                  ---     \r\n",
      "b512.fromrgb   256         16       [16, 64, 512, 512]   float16 \r\n",
      "b512.skip      8192        16       [16, 128, 256, 256]  float16 \r\n",
      "b512.conv0     36928       16       [16, 64, 512, 512]   float16 \r\n",
      "b512.conv1     73856       16       [16, 128, 256, 256]  float16 \r\n",
      "b512           -           16       [16, 128, 256, 256]  float16 \r\n",
      "b256.skip      32768       16       [16, 256, 128, 128]  float16 \r\n",
      "b256.conv0     147584      16       [16, 128, 256, 256]  float16 \r\n",
      "b256.conv1     295168      16       [16, 256, 128, 128]  float16 \r\n",
      "b256           -           16       [16, 256, 128, 128]  float16 \r\n",
      "b128.skip      131072      16       [16, 512, 64, 64]    float16 \r\n",
      "b128.conv0     590080      16       [16, 256, 128, 128]  float16 \r\n",
      "b128.conv1     1180160     16       [16, 512, 64, 64]    float16 \r\n",
      "b128           -           16       [16, 512, 64, 64]    float16 \r\n",
      "b64.skip       262144      16       [16, 512, 32, 32]    float16 \r\n",
      "b64.conv0      2359808     16       [16, 512, 64, 64]    float16 \r\n",
      "b64.conv1      2359808     16       [16, 512, 32, 32]    float16 \r\n",
      "b64            -           16       [16, 512, 32, 32]    float16 \r\n",
      "b32.skip       262144      16       [16, 512, 16, 16]    float32 \r\n",
      "b32.conv0      2359808     16       [16, 512, 32, 32]    float32 \r\n",
      "b32.conv1      2359808     16       [16, 512, 16, 16]    float32 \r\n",
      "b32            -           16       [16, 512, 16, 16]    float32 \r\n",
      "b16.skip       262144      16       [16, 512, 8, 8]      float32 \r\n",
      "b16.conv0      2359808     16       [16, 512, 16, 16]    float32 \r\n",
      "b16.conv1      2359808     16       [16, 512, 8, 8]      float32 \r\n",
      "b16            -           16       [16, 512, 8, 8]      float32 \r\n",
      "b8.skip        262144      16       [16, 512, 4, 4]      float32 \r\n",
      "b8.conv0       2359808     16       [16, 512, 8, 8]      float32 \r\n",
      "b8.conv1       2359808     16       [16, 512, 4, 4]      float32 \r\n",
      "b8             -           16       [16, 512, 4, 4]      float32 \r\n",
      "b4.mbstd       -           -        [16, 513, 4, 4]      float32 \r\n",
      "b4.conv        2364416     16       [16, 512, 4, 4]      float32 \r\n",
      "b4.fc          4194816     -        [16, 512]            float32 \r\n",
      "b4.out         513         -        [16, 1]              float32 \r\n",
      "---            ---         ---      ---                  ---     \r\n",
      "Total          28982849    480      -                    -       \r\n",
      "\r\n",
      "Setting up augmentation...\r\n",
      "Distributing across 2 GPUs...\r\n",
      "Setting up training phases...\r\n",
      "Exporting sample images...\r\n",
      "Initializing logs...\r\n",
      "2025-02-17 07:02:51.369379: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "2025-02-17 07:02:51.789309: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "2025-02-17 07:02:51.907726: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "Training for 15000 kimg...\r\n",
      "\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\r\n",
      "grad.sizes() = [1, 512], strides() = [1, 1]\r\n",
      "bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\r\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py:197: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\r\n",
      "grad.sizes() = [1, 512], strides() = [1, 1]\r\n",
      "bucket_view.sizes() = [1, 512], strides() = [512, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:325.)\r\n",
      "  Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "tick 0     kimg 0.0      time 4m 04s       sec/tick 130.7   sec/kimg 4084.17 maintenance 112.9  cpumem 4.24   gpumem 13.40  augment 0.000\r\n",
      "tick 1     kimg 4.0      time 15m 26s      sec/tick 664.6   sec/kimg 166.15  maintenance 18.1   cpumem 4.94   gpumem 13.72  augment 0.029\r\n",
      "tick 2     kimg 8.0      time 26m 33s      sec/tick 666.1   sec/kimg 166.52  maintenance 0.7    cpumem 4.94   gpumem 13.69  augment 0.064\r\n",
      "tick 3     kimg 12.0     time 37m 40s      sec/tick 666.4   sec/kimg 166.60  maintenance 0.7    cpumem 4.94   gpumem 13.74  augment 0.102\r\n",
      "tick 4     kimg 16.0     time 48m 46s      sec/tick 665.3   sec/kimg 166.33  maintenance 0.0    cpumem 4.94   gpumem 13.72  augment 0.132\r\n",
      "tick 5     kimg 20.0     time 59m 53s      sec/tick 666.9   sec/kimg 166.72  maintenance 0.7    cpumem 4.94   gpumem 13.72  augment 0.161\r\n",
      "tick 6     kimg 24.0     time 1h 11m 06s   sec/tick 658.3   sec/kimg 164.57  maintenance 14.5   cpumem 4.94   gpumem 13.76  augment 0.196\r\n",
      "tick 7     kimg 28.0     time 1h 22m 15s   sec/tick 668.0   sec/kimg 167.00  maintenance 0.7    cpumem 4.94   gpumem 13.76  augment 0.232\r\n",
      "tick 8     kimg 32.0     time 1h 33m 25s   sec/tick 670.4   sec/kimg 167.59  maintenance 0.0    cpumem 4.94   gpumem 13.82  augment 0.261\r\n",
      "tick 9     kimg 36.0     time 1h 44m 33s   sec/tick 667.2   sec/kimg 166.80  maintenance 0.7    cpumem 4.94   gpumem 13.79  augment 0.293\r\n",
      "tick 10    kimg 40.0     time 1h 55m 40s   sec/tick 666.7   sec/kimg 166.68  maintenance 0.7    cpumem 4.94   gpumem 13.83  augment 0.305\r\n",
      "tick 11    kimg 44.0     time 2h 06m 59s   sec/tick 663.4   sec/kimg 165.85  maintenance 15.3   cpumem 4.94   gpumem 13.76  augment 0.325\r\n",
      "tick 12    kimg 48.0     time 2h 18m 08s   sec/tick 669.2   sec/kimg 167.29  maintenance 0.0    cpumem 4.94   gpumem 13.82  augment 0.360\r\n",
      "tick 13    kimg 52.0     time 2h 29m 18s   sec/tick 668.8   sec/kimg 167.20  maintenance 0.7    cpumem 4.94   gpumem 13.79  augment 0.389\r\n",
      "tick 14    kimg 56.0     time 2h 40m 30s   sec/tick 671.8   sec/kimg 167.96  maintenance 0.7    cpumem 4.94   gpumem 13.89  augment 0.412\r\n",
      "tick 15    kimg 60.0     time 2h 51m 44s   sec/tick 673.1   sec/kimg 168.27  maintenance 0.8    cpumem 4.94   gpumem 13.83  augment 0.448\r\n",
      "tick 16    kimg 64.0     time 3h 03m 16s   sec/tick 676.6   sec/kimg 169.15  maintenance 15.0   cpumem 4.94   gpumem 13.86  augment 0.470\r\n",
      "tick 17    kimg 68.0     time 3h 14m 22s   sec/tick 664.9   sec/kimg 166.24  maintenance 0.9    cpumem 4.94   gpumem 13.84  augment 0.499\r\n",
      "tick 18    kimg 72.0     time 3h 25m 56s   sec/tick 693.6   sec/kimg 173.40  maintenance 0.8    cpumem 4.94   gpumem 13.93  augment 0.529\r\n",
      "tick 19    kimg 76.0     time 3h 38m 02s   sec/tick 725.1   sec/kimg 181.28  maintenance 0.7    cpumem 4.94   gpumem 13.85  augment 0.567\r\n",
      "tick 20    kimg 80.0     time 3h 50m 11s   sec/tick 729.6   sec/kimg 182.39  maintenance 0.0    cpumem 4.94   gpumem 14.04  augment 0.594\r\n",
      "tick 21    kimg 84.0     time 4h 02m 32s   sec/tick 724.4   sec/kimg 181.09  maintenance 15.7   cpumem 4.94   gpumem 13.90  augment 0.621\r\n",
      "tick 22    kimg 88.0     time 4h 14m 32s   sec/tick 719.9   sec/kimg 179.96  maintenance 0.7    cpumem 4.94   gpumem 13.92  augment 0.645\r\n",
      "tick 23    kimg 92.0     time 4h 26m 37s   sec/tick 724.3   sec/kimg 181.07  maintenance 0.8    cpumem 4.94   gpumem 13.92  augment 0.678\r\n",
      "tick 24    kimg 96.0     time 4h 38m 46s   sec/tick 728.5   sec/kimg 182.14  maintenance 0.0    cpumem 4.94   gpumem 13.92  augment 0.705\r\n",
      "tick 25    kimg 100.0    time 4h 50m 53s   sec/tick 727.0   sec/kimg 181.75  maintenance 0.7    cpumem 4.94   gpumem 14.01  augment 0.740\r\n",
      "tick 26    kimg 104.0    time 5h 03m 12s   sec/tick 724.0   sec/kimg 181.00  maintenance 15.0   cpumem 4.94   gpumem 13.94  augment 0.764\r\n",
      "tick 27    kimg 108.0    time 5h 15m 17s   sec/tick 724.0   sec/kimg 181.01  maintenance 0.7    cpumem 4.94   gpumem 13.92  augment 0.790\r\n",
      "tick 28    kimg 112.0    time 5h 27m 28s   sec/tick 731.0   sec/kimg 182.75  maintenance 0.0    cpumem 4.94   gpumem 13.90  augment 0.812\r\n",
      "tick 29    kimg 116.0    time 5h 39m 38s   sec/tick 728.7   sec/kimg 182.17  maintenance 0.7    cpumem 4.94   gpumem 14.00  augment 0.841\r\n",
      "tick 30    kimg 120.0    time 5h 51m 45s   sec/tick 727.1   sec/kimg 181.77  maintenance 0.7    cpumem 4.94   gpumem 13.89  augment 0.873\r\n",
      "tick 31    kimg 124.0    time 6h 04m 07s   sec/tick 726.9   sec/kimg 181.71  maintenance 15.2   cpumem 4.95   gpumem 13.94  augment 0.900\r\n",
      "tick 32    kimg 128.0    time 6h 16m 18s   sec/tick 730.2   sec/kimg 182.54  maintenance 0.0    cpumem 4.95   gpumem 14.03  augment 0.932\r\n",
      "tick 33    kimg 132.0    time 6h 28m 20s   sec/tick 721.7   sec/kimg 180.43  maintenance 0.8    cpumem 4.95   gpumem 13.97  augment 0.963\r\n",
      "tick 34    kimg 136.0    time 6h 40m 28s   sec/tick 726.7   sec/kimg 181.67  maintenance 0.7    cpumem 4.95   gpumem 13.99  augment 0.991\r\n",
      "tick 35    kimg 140.0    time 6h 52m 33s   sec/tick 724.5   sec/kimg 181.11  maintenance 0.7    cpumem 4.95   gpumem 13.97  augment 1.019\r\n",
      "tick 36    kimg 144.0    time 7h 04m 59s   sec/tick 731.4   sec/kimg 182.86  maintenance 14.5   cpumem 4.95   gpumem 13.96  augment 1.038\r\n",
      "tick 37    kimg 148.0    time 7h 17m 25s   sec/tick 745.6   sec/kimg 186.40  maintenance 0.7    cpumem 4.95   gpumem 14.08  augment 1.064\r\n",
      "tick 38    kimg 152.0    time 7h 29m 53s   sec/tick 747.6   sec/kimg 186.89  maintenance 0.7    cpumem 4.95   gpumem 14.00  augment 1.096\r\n",
      "tick 39    kimg 156.0    time 7h 42m 30s   sec/tick 756.1   sec/kimg 189.03  maintenance 0.7    cpumem 4.95   gpumem 13.93  augment 1.128\r\n",
      "tick 40    kimg 160.0    time 7h 55m 08s   sec/tick 757.5   sec/kimg 189.38  maintenance 0.0    cpumem 4.95   gpumem 14.00  augment 1.157\r\n",
      "tick 41    kimg 164.0    time 8h 07m 54s   sec/tick 749.5   sec/kimg 187.38  maintenance 16.3   cpumem 4.95   gpumem 13.98  augment 1.184\r\n",
      "tick 42    kimg 168.0    time 8h 20m 24s   sec/tick 749.5   sec/kimg 187.38  maintenance 0.7    cpumem 4.95   gpumem 13.94  augment 1.219\r\n",
      "tick 43    kimg 172.0    time 8h 32m 46s   sec/tick 741.6   sec/kimg 185.41  maintenance 0.7    cpumem 4.95   gpumem 13.92  augment 1.257\r\n",
      "tick 44    kimg 176.0    time 8h 45m 19s   sec/tick 753.1   sec/kimg 188.27  maintenance 0.0    cpumem 4.95   gpumem 13.92  augment 1.286\r\n",
      "tick 45    kimg 180.0    time 8h 57m 51s   sec/tick 751.1   sec/kimg 187.78  maintenance 0.7    cpumem 4.95   gpumem 13.94  augment 1.318\r\n",
      "tick 46    kimg 184.0    time 9h 10m 34s   sec/tick 748.2   sec/kimg 187.06  maintenance 14.9   cpumem 4.95   gpumem 14.06  augment 1.350\r\n",
      "tick 47    kimg 188.0    time 9h 23m 06s   sec/tick 750.9   sec/kimg 187.73  maintenance 0.7    cpumem 4.95   gpumem 13.97  augment 1.379\r\n",
      "tick 48    kimg 192.0    time 9h 35m 39s   sec/tick 753.2   sec/kimg 188.29  maintenance 0.0    cpumem 4.95   gpumem 14.01  augment 1.413\r\n",
      "tick 49    kimg 196.0    time 9h 48m 04s   sec/tick 744.2   sec/kimg 186.05  maintenance 0.8    cpumem 4.95   gpumem 13.98  augment 1.435\r\n",
      "tick 50    kimg 200.0    time 10h 00m 33s  sec/tick 747.9   sec/kimg 186.97  maintenance 0.7    cpumem 4.95   gpumem 14.01  augment 1.464\r\n",
      "tick 51    kimg 204.0    time 10h 13m 17s  sec/tick 748.0   sec/kimg 186.99  maintenance 15.8   cpumem 4.96   gpumem 14.01  augment 1.498\r\n",
      "tick 52    kimg 208.0    time 10h 25m 48s  sec/tick 751.1   sec/kimg 187.79  maintenance 0.0    cpumem 4.96   gpumem 13.94  augment 1.527\r\n",
      "tick 53    kimg 212.0    time 10h 38m 17s  sec/tick 748.9   sec/kimg 187.22  maintenance 0.7    cpumem 4.96   gpumem 13.98  augment 1.564\r\n",
      "tick 54    kimg 216.0    time 10h 50m 43s  sec/tick 744.6   sec/kimg 186.15  maintenance 0.7    cpumem 4.96   gpumem 14.08  augment 1.594\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/kaggle/working/stylegan2-ada-pytorch/train.py\", line 538, in <module>\r\n",
      "    main() # pylint: disable=no-value-for-parameter\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1157, in __call__\r\n",
      "    return self.main(*args, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1078, in main\r\n",
      "    rv = self.invoke(ctx)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 1434, in invoke\r\n",
      "    return ctx.invoke(self.callback, **ctx.params)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/core.py\", line 783, in invoke\r\n",
      "    return __callback(*args, **kwargs)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/click/decorators.py\", line 33, in new_func\r\n",
      "    return f(get_current_context(), *args, **kwargs)\r\n",
      "  File \"/kaggle/working/stylegan2-ada-pytorch/train.py\", line 533, in main\r\n",
      "    torch.multiprocessing.spawn(fn=subprocess_fn, args=(args, temp_dir), nprocs=args.num_gpus)\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 240, in spawn\r\n",
      "    return start_processes(fn, args, nprocs, join, daemon, start_method='spawn')\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 198, in start_processes\r\n",
      "    while not context.join():\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 160, in join\r\n",
      "    raise ProcessRaisedException(msg, error_index, failed_process.pid)\r\n",
      "torch.multiprocessing.spawn.ProcessRaisedException: \r\n",
      "\r\n",
      "-- Process 0 terminated with the following error:\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\r\n",
      "    fn(i, *args)\r\n",
      "  File \"/kaggle/working/stylegan2-ada-pytorch/train.py\", line 383, in subprocess_fn\r\n",
      "    training_loop.training_loop(rank=rank, **args)\r\n",
      "  File \"/kaggle/working/stylegan2-ada-pytorch/training/training_loop.py\", line 284, in training_loop\r\n",
      "    loss.accumulate_gradients(phase=phase.name, real_img=real_img, real_c=real_c, gen_z=gen_z, gen_c=gen_c, sync=sync, gain=gain)\r\n",
      "  File \"/kaggle/working/stylegan2-ada-pytorch/training/loss.py\", line 74, in accumulate_gradients\r\n",
      "    loss_Gmain.mean().mul(gain).backward()\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 487, in backward\r\n",
      "    torch.autograd.backward(\r\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 197, in backward\r\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\r\n",
      "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 516.00 MiB (GPU 0; 14.74 GiB total capacity; 12.35 GiB already allocated; 148.12 MiB free; 14.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!python train.py --resume=$pretrained_model --outdir=/kaggle/working/GANProject/Cracks \\\n",
    "    --data=/kaggle/input/class6-dataset-a --gpus=2 --cfg=paper512 --snap=5 --metrics=none \\\n",
    "    --augpipe=bgcf --mirror=1 --batch=32 --kimg=15000 "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 6677205,
     "sourceId": 10764383,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6679015,
     "sourceId": 10766857,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 237402,
     "modelInstanceId": 215694,
     "sourceId": 252292,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 237402,
     "modelInstanceId": 215694,
     "sourceId": 260198,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 237402,
     "modelInstanceId": 215694,
     "sourceId": 260648,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30887,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 39887.239116,
   "end_time": "2025-02-17T18:03:41.589034",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-17T06:58:54.349918",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
